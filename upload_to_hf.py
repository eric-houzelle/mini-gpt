"""
Script pour uploader un mod√®le Hugging Face sur le Hugging Face Hub.

Usage:
    python upload_to_hf.py --model_dir ./minigpt-hf --repo_id username/minigpt-fr --private
"""
import argparse
import os
from huggingface_hub import HfApi, create_repo, upload_folder
from transformers import AutoTokenizer, AutoModelForCausalLM


def main():
    parser = argparse.ArgumentParser(description="Uploader un mod√®le MiniGPT sur Hugging Face Hub")
    parser.add_argument(
        "--model_dir",
        type=str,
        required=True,
        help="R√©pertoire contenant le mod√®le Hugging Face"
    )
    parser.add_argument(
        "--repo_id",
        type=str,
        required=True,
        help="ID du repository (format: username/model-name)"
    )
    parser.add_argument(
        "--private",
        action="store_true",
        help="Cr√©er un repository priv√©"
    )
    parser.add_argument(
        "--token",
        type=str,
        default=None,
        help="Token Hugging Face (ou utiliser HF_TOKEN env var)"
    )
    parser.add_argument(
        "--commit_message",
        type=str,
        default="Upload MiniGPT model",
        help="Message de commit"
    )
    
    args = parser.parse_args()
    
    # Obtenir le token
    token = args.token or os.getenv("HF_TOKEN") or os.getenv("HUGGING_FACE_HUB_TOKEN")
    if not token:
        print("‚ùå Erreur: Token Hugging Face requis")
        print("   Fournissez --token ou d√©finissez la variable d'environnement HF_TOKEN")
        return
    
    print(f"üöÄ Upload du mod√®le sur Hugging Face Hub...")
    print(f"   Mod√®le: {args.model_dir}")
    print(f"   Repository: {args.repo_id}")
    print(f"   Priv√©: {args.private}")
    
    # V√©rifier que le mod√®le existe
    if not os.path.exists(args.model_dir):
        print(f"‚ùå Erreur: Le r√©pertoire {args.model_dir} n'existe pas")
        return
    
    # V√©rifier que les fichiers n√©cessaires existent
    required_files = ["config.json", "pytorch_model.bin"]
    if not os.path.exists(os.path.join(args.model_dir, "pytorch_model.bin")):
        # Peut-√™tre que c'est un mod√®le safetensors
        if not os.path.exists(os.path.join(args.model_dir, "model.safetensors")):
            print(f"‚ö†Ô∏è  Avertissement: Fichier de mod√®le non trouv√© dans {args.model_dir}")
            print(f"   V√©rifiez que le mod√®le a √©t√© correctement converti avec convert_to_hf.py")
    
    # Cr√©er le repository s'il n'existe pas
    print(f"\nüì¶ Cr√©ation du repository...")
    try:
        create_repo(
            repo_id=args.repo_id,
            token=token,
            private=args.private,
            exist_ok=True,
            repo_type="model"
        )
        print(f"   ‚úÖ Repository cr√©√©/v√©rifi√©")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur lors de la cr√©ation du repository: {e}")
        print(f"   (Le repository existe peut-√™tre d√©j√†, continuation...)")
    
    # Uploader les fichiers
    print(f"\nüì§ Upload des fichiers...")
    try:
        upload_folder(
            folder_path=args.model_dir,
            repo_id=args.repo_id,
            token=token,
            commit_message=args.commit_message,
            ignore_patterns=[".git", "__pycache__", "*.pyc"]
        )
        print(f"   ‚úÖ Upload r√©ussi!")
    except Exception as e:
        print(f"   ‚ùå Erreur lors de l'upload: {e}")
        return
    
    print(f"\n‚úÖ Mod√®le upload√© avec succ√®s!")
    print(f"\nüîó Lien: https://huggingface.co/{args.repo_id}")
    print(f"\nüí° Pour utiliser le mod√®le:")
    print(f"   from transformers import AutoModelForCausalLM")
    print(f"   model = AutoModelForCausalLM.from_pretrained('{args.repo_id}')")


if __name__ == "__main__":
    main()

